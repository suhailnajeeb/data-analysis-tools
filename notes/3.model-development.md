## Model Development

**Model**: A mathematical equation used to predict a value given one or more other values.

## Linear Regression

- Refers to one independent variable. 
- Predictor (Independent variable): x
- Target (Dependent variable): y

$y = b_0 + b_1 x$

$b_0$ : intercept
$b_1$ : slope

## Fitting a Simple Linear Model Estimator:

```python
X = df[['predictor']]
Y = df[['target']]
from sklearn.linear_model import LinearRegression
lm = LinearRegression()
lm.fit(X, Y)
Yhat = lm.predict(X)
```

View Intercept: ```lm.intercept_```
View Slope: ```lm.coef_```

## Multiple Linear Regression:

$ Y = b_0 + b_1 x_1 + b_2 x_2 + ... $

Used for: 
- One continuous variable
- Two/More Predictor variables

Syntax: similar.

```python
Z = df[['predictor1', 'predictor2', ...]]
Y = df[['target']]
from sklearn.linear_model import LinearRegression
lm = LinearRegression()
lm.fit(Z, Y)
Yhat = lm.predict(Z)
```

## Model Evaluation using Visualization

## Regression Plot

```python
import seaborn as sns
sns.regplot(x = "predictor", y = "target", data = df)
plt.ylim(0,)
```

## Residual Plot

```python
import seaborn as sns
sns.residplot(x, y)
```

## Distribution Plot

```python
import seaborn as sns
ax1 = sns.distplot(y, hist = False, color = "r", label = "Actual Value")
sns.distplot(Yhat, hist = False, color = "b", label = "Fitted Values", ax = ax1)
```

## Polynomial Regression

2nd order: 

$\hat{Y} = b_0 + b_1 x_1 + b_2 (x_1)^2$

3rd order: 

$\hat{Y} = b_0 + b_1 x_1 + b_2 (x_1)^2 + b_3 (x_1)^3$

```python
f = np.polyfit(x, y, 3)
```

Print out the model: 

```python
p = np.poly1d(f)
print(p)
```

## Polynomial Regression with More than One Dimension

$\hat{Y} = b_0 + b_1 X_1 + b_2 X_2 + b_3 X_1 X_2 + b_4 (X_1)^2 + b_5 (X_2)^2 + ... $

```python
from sklearn.preprocessing import PolynomialFeatures
pr = PolynomialFeatures(degree = 2)
pr.fit_transform([1,2], include_bias = False)
```

## Preprocessing: Normalization:

StandardScaler:

```python
from sklearn.preprocessing import StandardScaler
SCALE = StandardScaler()
SCALE.fit(x_data[['predictor1','predictor2']])
x_scale = SCALE.transform(x_data[['predictor1', 'predictor2']])
```

## Pipelines

```
Normalization > Polynomial Transform > Linear Regression
```

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler

Input = [('scale', StandardScaler()), ('polynomial', PolynomialFeatures(degree = 2)), ('mode', LinearRegression())]

#Pipeline Constructor
pipe = Pipeline(Input)

pipe.fit(Z, y)
yhat = pipe.predict(Z)
```

## Measures for In-Sample Evaluation

In-sample Evaluation: Numerically determine how good the model fits.

Two important measures: accuracy
- Mean Squared Error (MSE)
- R-Squared (R^2)

```python
from sklearn.metrics import mean_squared_error
mean_squared_error(Y, Yhat)
```

## R-Squared / R^2

- Coefficient of Determination
- Measure to determine how close the data is to the fitted regression line
- R^2: Percentage of variation explained by the linear model
- Basically comparing the regression model to the mean of the data points

$R^2 = (1 - \frac{MSE of regression line}{MSE of the average of the data})$

R2 scor of Linear Model:

```python
X = df[['predcitor']]
Y = df[['target']]

lm.fit(X, Y)
lm.score(X,Y)
```

## Prediction and Decision Making:

```python
lm.fit(X, Y)
lm.predict(np.array(30).reshape(-1,1))
```

```python
new_input = np.arange(1,101,1).reshape(-1,1)
yhat = lm.predict(new_input)
```

# Comparing MLR and SLR:

1. Lower MSE does not neccessarily imply a better fit.
2. MSE for MLR will always be smaller since errors will decrease with more variables included.
3. Polynomial Regression will have smaller MSE compared to regular regression.
4. Similar Inverse Relationship holds for R^2