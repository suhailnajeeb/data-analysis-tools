## Model Evaluation:

- In-sample evaluation
- Out-of sample evaluation

?

Solution: 

- In-sample Data/training
- Out-of-sample evaluation

## Training/Test Sets

The dataset is split into:

- training data
- testing data

```python
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.3, random_state = 0)
```

test_size : % of data used for test
random_state : random seed

## Generalization Error:

- Lots of Training Data: accuracy higher, precision lower
- Lots of Testing Data: Precision higher, accuracy lower

## Cross Validation:

```python
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, x_data, y_data, cv = 3)
cv_score = np.mean(scores)
```

## Cross Val Predict:

```python
from sklearn.model_selection import cross_val_predict
yhat = cross_val_predict(model, x_data, y_data, cv = 3)
```

## Overfitting, Underfitting and Model Selection:

Searching for the optimum degree of the polynomial:

```python
Rsqu_test = []
order = [1,2,3,4]

for n in order:
    pr = PolynomialFeatures(degree = n)
    x_train_pr = pr.fit_transform(x_train[['predictor']])
    x_test_pr = pr.fit_transform(x_test[['target']])
    lr.fit(x_train_pr, y_train)
    Rsqu_test.append(lr.score(x_test_pr, y_test))

```

## Ridge Regression:

```python
from sklearn.linear_model import Ridge
RidgeModel = Ridge(alpha = 0.1)
RidgeModel.fit(X,y)
Yhat = RidgeModel.predict(X)
```
So, we need to search for the optimum value of Alpha.


## Grid Search:

Searching for the optimum values of hyperparameters.

```python
from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV

parameters1 = [{'alpha':[0.001, 0.1, 1, 10, 100, 1000, 10000, 100000, 100000], 'normalize':[True, False]}]

RR = Ridge()

Grid1 = GridSearchCV(RR, parameters1, cv = 4)
Grid1.fit(X_data, Y_data)
Grid1.best_estimator_
scores = Grid1.cv_results_
scores['mean_test_score']
```